import json
import re
import time
import random
from base64 import b64decode
from io import BytesIO
from pathlib import Path
from urllib.parse import urljoin
from itertools import islice

import pandas as pd
from PIL import Image
from playwright.sync_api import (
    sync_playwright,
    Page,
    TimeoutError as PWTimeoutError,
    Error as PWError,
)

# ========== –ù–ê–°–¢–†–û–ô–ö–ò ==========

# –§–∞–π–ª —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ (Excel/CSV/TXT). –£–∫–∞–∂–∏ —Å–≤–æ–π —Ñ–∞–π–ª:
INPUT_FILE = Path("–ê–í–¢–û–°–ê–õ–û–ù 09.11 2000.xlsx")
INPUT_SHEET = None        # None = –≤—Å–µ –ª–∏—Å—Ç—ã; –ª–∏–±–æ –∏–º—è/–∏–Ω–¥–µ–∫—Å
URL_COLUMN = None         # None = –∏—Å–∫–∞—Ç—å –ø–æ –≤—Å–µ–º –∫–æ–ª–æ–Ω–∫–∞–º regex-–æ–º; –ª–∏–±–æ –∏–º—è –∫–æ–ª–æ–Ω–∫–∏

OUT_DIR = Path("avito_phones_playwright")
OUT_DIR.mkdir(exist_ok=True)
IMG_DIR = OUT_DIR / "phones"
IMG_DIR.mkdir(exist_ok=True)

SAVE_DATA_URI = True      # True -> —Å–æ—Ö—Ä–∞–Ω—è–µ–º data:image... –≤ JSON; False -> –ø—É—Ç—å –∫ PNG
HEADLESS = False          # –Ω—É–∂–Ω–æ False: –ª–æ–≥–∏–Ω —Ä—É–∫–∞–º–∏
CONCURRENCY = 3           # —Å–∫–æ–ª—å–∫–æ –≤–∫–ª–∞–¥–æ–∫ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ (2‚Äì4 –±–µ–∑–æ–ø–∞—Å–Ω–æ)
MAX_ITEMS = None          # –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª-–≤–æ —Å—Å—ã–ª–æ–∫; None = –≤—Å–µ
CLICK_DELAY = 8           # –æ–∂–∏–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ –∫–ª–∏–∫–∞ "–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω", —Å–µ–∫
NAV_TIMEOUT = 90_000

USE_PROXY = False
PROXY_HOST = "mproxy.site"
PROXY_PORT = 17518
PROXY_LOGIN = "YT4aBK"
PROXY_PASSWORD = "nUg2UTut9UMU"

PAGE_DELAY_BETWEEN_BATCHES = (2.0, 4.0)  # –ø–∞—É–∑–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
      "AppleWebKit/537.36 (KHTML, like Gecko) "
      "Chrome/120.0.0.0 Safari/537.36")


# ========== –•–ï–õ–ü–ï–†–´ ==========

def human_sleep(a: float, b: float):
    time.sleep(random.uniform(a, b))


def safe_get_content(page: Page) -> str:
    try:
        return page.content()
    except PWError:
        time.sleep(0.8)
        try:
            return page.content()
        except PWError:
            return ""


def is_captcha_or_block(page: Page) -> bool:
    try:
        url = page.url.lower()
    except PWError:
        url = ""
    html = safe_get_content(page).lower()
    if "captcha" in url or "firewall" in url:
        return True
    if "–¥–æ—Å—Ç—É–ø —Å –≤–∞—à–µ–≥–æ ip-–∞–¥—Ä–µ—Å–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω" in html:
        return True
    return False


def close_city_or_cookie_modals(page: Page):
    selectors = [
        "button[aria-label='–ó–∞–∫—Ä—ã—Ç—å']",
        "button[data-marker='modal-close']",
        "button[class*='close']",
        "button:has-text('–ü–æ–Ω—è—Ç–Ω–æ')",
        "button:has-text('–•–æ—Ä–æ—à–æ')",
    ]
    for sel in selectors:
        try:
            for b in page.query_selector_all(sel):
                if b.is_visible():
                    b.click()
                    human_sleep(0.25, 0.7)
        except Exception:
            continue


def close_login_modal_if_exists(page: Page) -> bool:
    """–ï—Å–ª–∏ –≤—Å–ø–ª—ã–ª–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è ‚Äî –∑–∞–∫—Ä—ã–≤–∞–µ–º –∏ —Å—á–∏—Ç–∞–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –Ω–µ—É–¥–∞—á–Ω—ã–º."""
    selectors_modal = [
        "[data-marker='login-form']",
        "[data-marker='registration-form']",
        "div[class*='modal'][class*='auth']",
        "div[class*='modal'] form[action*='login']",
    ]
    for sel in selectors_modal:
        try:
            modals = page.query_selector_all(sel)
        except PWError:
            continue

        for m in modals:
            if not m.is_visible():
                continue

            for btn_sel in [
                "button[aria-label='–ó–∞–∫—Ä—ã—Ç—å']",
                "button[data-marker='modal-close']",
                "button[class*='close']",
                "button[type='button']",
            ]:
                btn = m.query_selector(btn_sel)
                if btn and btn.is_enabled():
                    try:
                        btn.click()
                        human_sleep(0.3, 0.6)
                        print("üîí –ú–æ–¥–∞–ª–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–∫—Ä—ã—Ç–∞, –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ.")
                        return True
                    except Exception:
                        pass

            print("üîí –ú–æ–¥–∞–ª–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è ‚Äî –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            return True

    return False


def save_phone_png_from_data_uri(data_uri: str, file_stem: str) -> str | None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É –∏–∑ data:image/... –≤ phones/{file_stem}.png"""
    try:
        header, b64_data = data_uri.split(",", 1)
        raw = b64decode(b64_data)
        image = Image.open(BytesIO(raw)).convert("RGB")
        file_name = f"{file_stem}.png"
        out_path = IMG_DIR / file_name
        image.save(out_path)
        print(f"üíæ PNG —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {out_path}")
        return str(out_path)
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ PNG: {e}")
        return None


def get_avito_id_from_url(url: str) -> str:
    """–ü—ã—Ç–∞–µ—Ç—Å—è –≤—ã—Ç–∞—â–∏—Ç—å —á–∏—Å–ª–æ–≤–æ–π ID –∏–∑ URL –æ–±—ä—è–≤–ª–µ–Ω–∏—è."""
    m = re.search(r'(\d{7,})', url)
    return m.group(1) if m else str(int(time.time()))


def click_show_phone_on_ad(page: Page) -> bool:
    """–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏—â–µ—Ç –∏ –∫–ª–∏–∫–∞–µ—Ç –∫–Ω–æ–ø–∫—É '–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω/–Ω–æ–º–µ—Ä'."""
    btn_selectors = [
        "button[data-marker='item-phone-button']",
        "button:has-text('–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω')",
        "button:has-text('–ü–æ–∫–∞–∑–∞—Ç—å –Ω–æ–º–µ—Ä')",
        "button[aria-label*='–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω']",
        "button[aria-label*='–ü–æ–∫–∞–∑–∞—Ç—å –Ω–æ–º–µ—Ä']",
    ]
    for sel in btn_selectors:
        try:
            b = page.query_selector(sel)
            if b and b.is_enabled() and b.is_visible():
                b.scroll_into_view_if_needed()
                human_sleep(0.25, 0.6)
                b.click()
                print("üìû –ù–∞–∂–∞–ª–∏ '–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω'.")
                return True
        except Exception:
            continue
    print("‚ö†Ô∏è –ö–Ω–æ–ø–∫–∞ '–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    return False


def extract_phone_data_uri_on_ad(page: Page) -> str | None:
    """
    –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏—â–µ—Ç img[data-marker='phone-image'],
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç data:image/png;base64,....
    """
    try:
        img = page.query_selector("img[data-marker='phone-image']")
    except PWError:
        img = None

    if not img or not img.is_visible():
        print("‚ö†Ô∏è –ö–∞—Ä—Ç–∏–Ω–∫–∞ —Å –Ω–æ–º–µ—Ä–æ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        return None

    src = img.get_attribute("src") or ""
    if not src.startswith("data:image"):
        print(f"‚ö†Ô∏è src –Ω–µ data:image, –∞: {src[:60]}...")
        return None
    return src


def read_urls_from_excel_or_csv(
    path: Path,
    sheet: str | int | None = None,
    url_column: str | None = None
) -> list[str]:
    url_re = re.compile(r'https?://(?:www\.)?avito\.ru/[^\s"]+')
    urls: list[str] = []

    if path.suffix.lower() in {".xlsx", ".xls"}:
        xls = pd.ExcelFile(path)
        sheets = [sheet] if sheet is not None else xls.sheet_names
        for sh in sheets:
            df = xls.parse(sh, dtype=str)
            if url_column and url_column in df.columns:
                col = df[url_column].dropna().astype(str)
                urls.extend(col.tolist())
            else:
                for col in df.columns:
                    s = df[col].dropna().astype(str)
                    for val in s:
                        urls.extend(url_re.findall(val))
    elif path.suffix.lower() in {".csv", ".txt"}:
        df = pd.read_csv(path, dtype=str, sep=None, engine="python")
        if url_column and url_column in df.columns:
            col = df[url_column].dropna().astype(str)
            urls.extend(col.tolist())
        else:
            for col in df.columns:
                s = df[col].dropna().astype(str)
                for val in s:
                    urls.extend(url_re.findall(val))
    else:
        raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è .xlsx/.xls/.csv/.txt")

    # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
    cleaned = []
    seen = set()
    for u in urls:
        u = u.strip()
        if not u.startswith("http"):
            u = urljoin("https://www.avito.ru", u)
        u = u.split("#", 1)[0]
        u = u.split("?", 1)[0]
        if u not in seen:
            seen.add(u)
            cleaned.append(u)
    return cleaned


def batched(iterable, n):
    it = iter(iterable)
    while True:
        batch = list(islice(it, n))
        if not batch:
            return
        yield batch


def process_batch(context, batch_urls):
    """
    –û—Ç–∫—Ä—ã–≤–∞–µ—Ç –ø–∞—á–∫—É –≤–∫–ª–∞–¥–æ–∫, –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç –ø–æ URL, –∫–ª–∏–∫–∞–µ—Ç '–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω',
    –∂–¥—ë—Ç CLICK_DELAY, —Å–æ–±–∏—Ä–∞–µ—Ç data:image... –∏ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç –≤–∫–ª–∞–¥–∫–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict[url] = data_uri | png_path | None
    """
    results: dict[str, str] = {}
    pages: list[tuple[str, Page]] = []
    try:
        # 1) –û—Ç–∫—Ä—ã–ª–∏ –≤–∫–ª–∞–¥–∫–∏ –∏ –ø–µ—Ä–µ—à–ª–∏ –ø–æ URL
        for url in batch_urls:
            p = context.new_page()
            pages.append((url, p))
            try:
                p.goto(url, wait_until="load", timeout=NAV_TIMEOUT)
            except PWTimeoutError:
                print(f"‚ö†Ô∏è –ù–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: {url}")
            human_sleep(0.2, 0.6)

        # 2) –ù–∞ –∫–∞–∂–¥–æ–π –≤–∫–ª–∞–¥–∫–µ ‚Äî –º–æ–¥–∞–ª–∫–∏/–∫–Ω–æ–ø–∫–∞
        for url, p in pages:
            if is_captcha_or_block(p):
                print(f"üö´ –ö–∞–ø—á–∞/–±–ª–æ–∫ –Ω–∞ {url}")
                continue
            close_city_or_cookie_modals(p)
            if not click_show_phone_on_ad(p):
                continue

        # 3) –ñ–¥—ë–º –æ—Ç—Ä–∏—Å–æ–≤–∫—É –∫–∞—Ä—Ç–∏–Ω–æ–∫ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        time.sleep(CLICK_DELAY)

        # 4) –°–±–æ—Ä –∫–∞—Ä—Ç–∏–Ω–æ–∫ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        for url, p in pages:
            if close_login_modal_if_exists(p) or is_captcha_or_block(p):
                continue
            data_uri = extract_phone_data_uri_on_ad(p)
            if not data_uri:
                continue

            if SAVE_DATA_URI:
                results[url] = data_uri
                print(f"‚úÖ {url} -> [data:image...]")
            else:
                avito_id = get_avito_id_from_url(url)
                out_path = save_phone_png_from_data_uri(data_uri, avito_id)
                if out_path:
                    results[url] = out_path
                    print(f"‚úÖ {url} -> {out_path}")

    finally:
        # 5) –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ –≤–∫–ª–∞–¥–∫–∏ –ø–∞–∫–µ—Ç–∞
        for _, p in pages:
            try:
                p.close()
            except Exception:
                pass

    return results


# ========== –û–°–ù–û–í–ù–û–ô –°–¶–ï–ù–ê–†–ò–ô ==========

def main():
    urls = read_urls_from_excel_or_csv(INPUT_FILE, INPUT_SHEET, URL_COLUMN)
    if MAX_ITEMS:
        urls = urls[:MAX_ITEMS]
    print(f"üîé –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(urls)}")
    if not urls:
        return

    with sync_playwright() as p:
        launch_kwargs = {
            "headless": HEADLESS,
            "args": [
                "--disable-blink-features=AutomationControlled",
                "--start-maximized",
            ],
        }
        if USE_PROXY:
            launch_kwargs["proxy"] = {
                "server": f"http://{PROXY_HOST}:{PROXY_PORT}",
                "username": PROXY_LOGIN,
                "password": PROXY_PASSWORD,
            }

        browser = p.chromium.launch(**launch_kwargs)
        context = browser.new_context(
            viewport={"width": 1280, "height": 800},
            user_agent=UA,
        )
        context.set_default_navigation_timeout(NAV_TIMEOUT)
        context.set_default_timeout(NAV_TIMEOUT)

        # --- –†–£–ß–ù–û–ô –õ–û–ì–ò–ù –Ω–∞ 1-–π —Å—Å—ã–ª–∫–µ ---
        page = context.new_page()
        try:
            page.goto(urls[0], wait_until="load", timeout=NAV_TIMEOUT)
        except PWTimeoutError:
            pass

        print("\nüîë –¢–≤–æ–∏ –¥–µ–π—Å—Ç–≤–∏—è:")
        print("   ‚Ä¢ –µ—Å–ª–∏ –µ—Å—Ç—å –∫–∞–ø—á–∞ ‚Äî —Ä–µ—à–∏;")
        print("   ‚Ä¢ –∑–∞–ª–æ–≥–∏–Ω—å—Å—è –≤ –ê–≤–∏—Ç–æ;")
        print("   ‚Ä¢ –æ—Å—Ç–∞–≤—å –æ—Ç–∫—Ä—ã—Ç—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è.")
        input("üëâ –ì–æ—Ç–æ–≤? –ù–∞–∂–º–∏ Enter –≤ –∫–æ–Ω—Å–æ–ª–∏.\n")

        if is_captcha_or_block(page):
            print("‚ùå –í—Å—ë –µ—â—ë –∫–∞–ø—á–∞/–±–ª–æ–∫ ‚Äî –≤—ã—Ö–æ–¥–∏–º.")
            browser.close()
            return

        # –ú–æ–∂–Ω–æ –∑–∞–∫—Ä—ã—Ç—å —Å—Ç–∞—Ä—Ç–æ–≤—É—é –≤–∫–ª–∞–¥–∫—É –ª–æ–≥–∏–Ω–∞
        try:
            page.close()
        except Exception:
            pass

        phones_map: dict[str, str] = {}

        # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞–º–∏ –≤–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∫–ª–∞–¥–∫–∞—Ö ---
        for batch_urls in batched(urls, CONCURRENCY):
            try:
                res = process_batch(context, batch_urls)
                phones_map.update(res)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–∞–∫–µ—Ç–∞: {e}")
            human_sleep(*PAGE_DELAY_BETWEEN_BATCHES)

        browser.close()

        # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ---
        out_file = OUT_DIR / "phones_map.json"
        out_file.write_text(json.dumps(phones_map, ensure_ascii=False, indent=2), encoding="utf-8")
        print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ. –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(phones_map)} –∑–∞–ø–∏—Å–µ–π –≤ {out_file}")
        if not SAVE_DATA_URI:
            print(f"üìÇ PNG –ª–µ–∂–∞—Ç –≤ {IMG_DIR}")


if __name__ == "__main__":
    main()
